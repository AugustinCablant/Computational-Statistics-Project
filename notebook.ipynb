{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational inference via Wasserstein gradient flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from distances import W2, KL_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VARIATIONAL INFERENCE WITH GAUSSIANS\n",
    "\n",
    "### Buresâ€“JKO scheme\n",
    "\n",
    "Let $p_0 = \\mathcal{N} (m_0, \\sigma _0) $ and $p_1 = \\mathcal{N} (m_1, \\sigma _1)$ be two Gaussian distributions. \n",
    "\n",
    "Then, the Wasserstein distance $W _2 ^2 (p_0, p_1)$ admits a close form  : $W _2 ^2 (p_0, p_1) = || m_0 - m_1 || ^2 + \\mathcal{B} (\\sigma _0, \\sigma _1)$, where $B(.,.)$ is the squared Bures metric. \n",
    "\n",
    "Now, given a target density $ \\pi $ (which will be Gaussian in our case) we define the iterates of the proximal point algorithm : $ p_{k+1, h} = \\text{argmin }_p \\left[ \\text{ KL}(p || \\pi) + \\frac{1}{2h} W _2 ^2 (p, p_{k,h}) \\right] $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10 ** 4 \n",
    "d = 2 \n",
    "target_distribution = stats.multivariate_normal(mean = [3, 2], cov = [[6, 1], [1, 3]])\n",
    "\n",
    "def Bures_JKO_scheme(target_distribution, N, d, MaxIter = 10 * 3):\n",
    "    \"\"\" \n",
    "    Bures JKO scheme for sampling from a target distribution.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    target_distribution: scipy.stats distribution\n",
    "        The target distribution from which we want to sample from.\n",
    "\n",
    "    N: int\n",
    "        The number of samples to generate.\n",
    "    \n",
    "    d: int\n",
    "        The dimension of the target distribution.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    samples: np.array\n",
    "        The samples generated from the target\n",
    "    \"\"\"\n",
    "    h = 1 / N    # step size\n",
    "    pi = target_distribution.sample(N)    # samples from the target distribution\n",
    "    p_0_h = stats.multivariate_normal(mean = [0] * d, cov = np.eye(d)).sample(N)    # initial distribution\n",
    "    for _ in range(MaxIter):\n",
    "        # Here we have a problem : how to compute an argmin when we are working with probability distributions ?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
